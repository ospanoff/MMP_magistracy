{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Лабораторная работа №2 (курс \"Математические методы анализа текстов\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Тема: Языковое моделирование и определение языка.\n",
    "\n",
    "\n",
    "**Выдана**:   13 марта 2017\n",
    "\n",
    "**Дедлайн**:   <font color='red'>9:00 утра 27 марта 2017</font>\n",
    "\n",
    "**Среда выполнения**: Jupyter Notebook (Python 2.7)\n",
    "\n",
    "#### Правила:\n",
    "\n",
    "Результат выполнения задания $-$ отчет в формате Jupyter Notebook с кодом и выводами. В ходе выполнения задания требуется реализовать все необходимые алгоритмы, провести эксперименты и ответить на поставленные вопросы. Дополнительные выводы приветствуются. Чем меньше кода и больше комментариев $-$ тем лучше.\n",
    "\n",
    "Все ячейки должны быть \"выполненными\", при этом результат должен воспроизвдиться при проверке (на Python 2.7). Если какой-то код не был запущен или отрабатывает с ошибками, то пункт не засчитывается. Задание, сданное после дедлайна, _не принимается_. Совсем.\n",
    "\n",
    "\n",
    "Задание выполняется самостоятельно. Вы можете обсуждать идеи, объяснять друг другу материал, но не можете обмениваться частями своего кода. Если какие-то студенты будут уличены в списывании, все они автоматически получат за эту работу 0 баллов, а также предвзято негативное отношение семинаристов в будущем. Если вы нашли в Интернете какой-то код, который собираетесь заимствовать, обязательно укажите это в задании: вполне вероятно, что вы не единственный, кто найдёт и использует эту информацию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Постановка задачи:\n",
    "\n",
    "В данной лабораторной работе Вам предстоит реализовать n-грамную языковую модель с несколькими видами сглаживания:\n",
    "- Add-one smoothing\n",
    "- Stupid backoff\n",
    "- Interpolation smoothing\n",
    "- Kneser-Ney smoothing\n",
    "\n",
    "Вы обучите ее на готовых корпусах, оцените качество и проведете ряд экспериментов. Во второй части задания Вы примените реализованную модель (но с буквенными n-граммами) к задаче распознавания языка. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Языковые модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Цель языкового моделирования заключается в том, чтобы присвоить некоторые вероятности предложениям. Задача состоит в подсчете вероятности $P(W) = P(w_1, \\dots, w_n)$ или $P(w_n \\mid w_1, \\dots, w_{n-1})$. Модель, умеющая вычислять хотя бы одну из этих двух вероятностей, называется **языковой моделью** (LM от Language Model).\n",
    "\n",
    "Согласно **цепному правилу** (chain rule):\n",
    "\n",
    "$$P(X_1, \\dots, X_n) = P(X_1)P(X_2 \\mid X_1)\\dots P(X_n \\mid X_1, \\dots, X_{n-1}).$$ \n",
    "\n",
    "Также мы знаем, что\n",
    "\n",
    "$$\n",
    "    P(X_n \\mid X_1, \\dots, X_{n-1}) = \\frac{P(X_1, \\dots, X_n)}{P(X_1, \\dots, X_{n-1})},\n",
    "$$\n",
    "\n",
    "следовательно, для того чтобы оценить $P(X_n \\mid X_1, \\dots, X_{n-1})$ нужно посчитать $P(X_1, \\dots, X_n)$ и $P(X_1, \\dots, X_{n-1})$. Но эти вероятности будут чрезвычайно малы, если мы возьмем большое $n$, так множество предложений из $n$ слов растет экспоненциально. Для упрощения применим **марковское предположение**: \n",
    "\n",
    "$$P(X_n \\mid X_1, \\dots, X_{n-1}) = P(X_n \\mid X_{n - k + 1}, \\dots, X_{n-1})$$\n",
    "\n",
    "для некоторого фиксированного (небольшого) $k$. Это предположение говорит о том, что $X_{n}$ не зависит от $X_{1}, \\dots, X_{n - k}$, то есть на следующее слово влияет лишь контекст из предыдущих $k - 1$ слова. Таким образом, мы получаем финальную вероятность:\n",
    "\n",
    "$$\n",
    "    P(w_1, \\dots, w_n) = \\prod_i P(w_i \\mid w_{i-k+1}, \\dots, w_{i - 1}).\n",
    "$$\n",
    "\n",
    "Далее для краткости будем обозначать $w_{i-k}^i := w_{i-k}, \\dots, w_{i}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Хранилище n-грам"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Для начала выполним вспомогательную работу. Следуйте комментариям, чтобы написать NGramStorage с удобным интерфейсом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ospanoff/.pyenv/versions/2.7.11/envs/python2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import math\n",
    "import random\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class NGramStorage:\n",
    "    \"\"\"Storage for ngrams' frequencies.\n",
    "    \n",
    "    Args:\n",
    "        sents (list[list[str]]): List of sentences from which ngram\n",
    "            frequencies are extracted.\n",
    "        max_n (int): Upper bound of the length of ngrams.\n",
    "            For instance if max_n = 2, then storage will store\n",
    "            0, 1, 2-grams.\n",
    "            \n",
    "    Attributes:\n",
    "        max_n (Readonly(int)): Upper bound of the length of ngrams.\n",
    "    \"\"\"\n",
    "        \n",
    "    def __init__(self, sents=[], max_n=0):\n",
    "        # self._ngrams[K] should have the following interface:\n",
    "        # self._ngrams[K][(w_1, ..., w_K)] = number of times w_1, ..., w_K occured in words\n",
    "        # self._ngrams[0][()] = number of all words\n",
    "        self.__max_n = max_n\n",
    "        self.__ngrams = {\n",
    "            i: Counter(tuple(sent[j: j + i]) for sent in sents for j in range(len(sent) - i + (i > 0)))\n",
    "            for i in range(self.__max_n + 1)\n",
    "        }\n",
    "        \n",
    "    def add_unk_token(self):\n",
    "        \"\"\"Add UNK token to 1-grams.\"\"\"\n",
    "        # In order to avoid zero probabilites \n",
    "        if self.__max_n == 0 or u'UNK' in self.__ngrams[1]:\n",
    "            return\n",
    "        self.__ngrams[0][()] += 1\n",
    "        self.__ngrams[1][(u'UNK',)] = 1\n",
    "        \n",
    "    @property\n",
    "    def max_n(self):\n",
    "        \"\"\"Get max_n\"\"\"\n",
    "        return self.__max_n\n",
    "        \n",
    "    def __getitem__(self, k):\n",
    "        \"\"\"Get dictionary of k-gram frequencies.\n",
    "        \n",
    "        Args:\n",
    "            k (int): length of returning ngrams' frequencies.\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary (in fact Counter) of k-gram frequencies.\n",
    "        \"\"\"\n",
    "        # Cheking the input\n",
    "        if not isinstance(k, int):\n",
    "            raise TypeError('k (length of ngrams) must be an integer!')\n",
    "        if k > self.__max_n:\n",
    "            raise ValueError('k (length of ngrams) must be less or equal to the maximal length!')\n",
    "        return self.__ngrams[k]\n",
    "    \n",
    "    def __call__(self, ngram):\n",
    "        \"\"\"Return frequency of a given ngram.\n",
    "        \n",
    "        Args:\n",
    "            ngram (tuple): ngram for which frequency should be computed.\n",
    "            \n",
    "        Returns:\n",
    "            Frequency (int) of a given ngram.\n",
    "        \"\"\"\n",
    "        # Cheking the input\n",
    "        if not isinstance(ngram, tuple):\n",
    "            raise TypeError('ngram must be a tuple!')\n",
    "        if len(ngram) > self.__max_n:\n",
    "            raise ValueError('length of ngram must be less or equal to the maximal length!')\n",
    "        if len(ngram) == 1 and ngram not in self.__ngrams[1]:\n",
    "            return self.__ngrams[1][(u'UNK', )]\n",
    "        return self.__ngrams[len(ngram)][ngram]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Оценка качества"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Скачайте brown корпус, обучите модель и протестируйте на нескольких примерах последовательностей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# Uncomment next row and download brown corpus\n",
    "# nltk.download()\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of all sentences = 57340\n",
      "Number of train sentences = 45872\n",
      "Number of test sentences = 11468\n"
     ]
    }
   ],
   "source": [
    "all_sents = list(brown.sents())\n",
    "random.shuffle(all_sents)\n",
    "print('Number of all sentences = {}'.format(len(all_sents)))\n",
    "train_sents = all_sents[:int(0.8 * len(all_sents))]\n",
    "test_sents = all_sents[int(0.8 * len(all_sents)):]\n",
    "print('Number of train sentences = {}'.format(len(train_sents)))\n",
    "print('Number of test sentences = {}'.format(len(test_sents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create storage of 0, 1, 2, 3-grams\n",
    "storage = NGramStorage(train_sents, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1385\n",
      "3253\n",
      "26\n",
      "0\n",
      "928114\n"
     ]
    }
   ],
   "source": [
    "# It's time to test your code\n",
    "print(storage(('to', 'be')))\n",
    "print(storage(('or',)))\n",
    "print(storage(('not', 'to', 'be')))\n",
    "print(storage(('somethingweird',)))\n",
    "print(storage(()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Для численного измерения качества языковой модели определим **перплексию**:\n",
    "\n",
    "$$\n",
    "    {\\mathbb{P}}(w_1, \\dots, w_N) = P(w_1, \\dots, w_N)^{-\\frac1N} = \\left( \\prod_i P(w_i \\mid w_{i - k}, \\dots, w_{i - 1})\\right)^{-\\frac1N},\n",
    "$$\n",
    "\n",
    "Вижно, что минимизация перплексии эквивалентна максимизации правдоподобия модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Реализуйте функцию по подсчету перплексии. Обратите внимание, что перплексия по корпусу равна произведению вероятностей **всех** предложений в степени $-\\frac1N$, где $N -$ суммарная длина всех предложений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def perplexity(estimator, sents):\n",
    "    '''Estimate perplexity of the sequence of words using prob_estimator.'''\n",
    "    perp = 0\n",
    "    N = 0\n",
    "    for sent in sents:\n",
    "        N += len(sent)\n",
    "        perp += np.log(estimator.prob(sent) + 1e-50)\n",
    "    \n",
    "    return np.exp(-perp / N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Оценка вероятностей n-грам"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Первый и простейший способ оценки вероятностей N-грам следующий:\n",
    "\n",
    "$$\n",
    "    \\hat P_{S}(w_{N} \\mid w_1^{N - 1}) = \\frac{c(w_1^N)}{c(w_1^{N-1})}.\n",
    "$$\n",
    "\n",
    "где $c(w_1^N)$ — это число последовательностей $w_1, \\dots, w_N$ в корпусе, $S$ символизирует Straightforward. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class StraightforwardProbabilityEstimator:\n",
    "    \"\"\"Class for simplest probability estimations of type P(word | context).\n",
    "    \n",
    "    P(word | context) = c(context + word) / c(context), where\n",
    "    c(sequence) - number of occurances of the sequence in the corpus.\n",
    "    \n",
    "    Args:\n",
    "        storage(NGramStorage): Object of NGramStorage class which will\n",
    "            be used to extract frequencies of ngrams.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, storage):\n",
    "        self.__storage = storage\n",
    "        # Adding UNK token to avoid zero probabilities\n",
    "        self.__storage.add_unk_token()\n",
    "        \n",
    "    def cut_context(self, context):\n",
    "        \n",
    "        \"\"\"Cut context if it is too large.\n",
    "        \n",
    "        Args:\n",
    "            context (tuple[str]): Some sequence of words.\n",
    "        \n",
    "        Returns:\n",
    "            Cutted context (tuple[str]) up to the length of max_n.\n",
    "        \"\"\"\n",
    "        if self.__storage.max_n == 1:\n",
    "            return ()\n",
    "        if len(context) + 1 > self.__storage.max_n:\n",
    "            context = context[-self.__storage.max_n + 1:]\n",
    "        return context\n",
    "        \n",
    "    def __call__(self, word, context):\n",
    "        \"\"\"Estimate conditional probability P(word | context).\n",
    "        \n",
    "        Args:\n",
    "            word (str): Current word.\n",
    "            context (tuple[str]): Context of a word.\n",
    "            \n",
    "        Returns:\n",
    "            Conditional probability (float) P(word | context).\n",
    "        \"\"\"\n",
    "        # Cheking the input\n",
    "        if not isinstance(word, (unicode, str)):\n",
    "            raise TypeError('word must be a string!')\n",
    "        if not isinstance(context, tuple):\n",
    "            raise TypeError('word must be a tuple!')\n",
    "        # If context is too large, let's cut it.\n",
    "        context = self.cut_context(context)\n",
    "        phrase_counts = self.__storage(context + (word, ))\n",
    "        context_counts = self.__storage(context)\n",
    "        # Avoiding 0 / 0.\n",
    "        if context_counts == 0:\n",
    "            return 0.\n",
    "        return 1. * phrase_counts / context_counts\n",
    "    \n",
    "    def prob(self, sent):\n",
    "        \"\"\"Estimate probability of a sentence using Markov rule.\n",
    "        \n",
    "        Args:\n",
    "            sentence (list[str]): Sentence for probability estimation.\n",
    "            \n",
    "        Returns:\n",
    "            Probability (float) P(sentence).\n",
    "        \"\"\"\n",
    "        prob = 1.\n",
    "        for i in range(len(sent)):\n",
    "            prob *= self(sent[i], tuple(sent[:i]))\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple estimator perplexity = 250.415036669\n",
      "1.5084337609e-05\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# Initialize estimator\n",
    "simple_estimator = StraightforwardProbabilityEstimator(storage)\n",
    "\n",
    "# Estimating perplexity\n",
    "print('Simple estimator perplexity = {}'.format(perplexity(simple_estimator, test_sents)))\n",
    "print(simple_estimator.prob('To be'.split()))\n",
    "print(simple_estimator.prob('To be or not to be'.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be...\n",
      "\n",
      "To be Katharine Ross ,\n",
      "To be human , he\n",
      "To be eligible to borrow\n",
      "To be fit , one\n",
      "To be checked out further\n",
      "To be sure of matching\n",
      "To be sure , in\n",
      "To be on the safe\n",
      "To be presiding officer of\n",
      "To be passive , to\n",
      "To be reminded of this\n",
      "\n",
      "To be or...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print 'To be...\\n'\n",
    "for sent in train_sents:\n",
    "    if sent[:2] == ['To', 'be']:\n",
    "        print ' '.join(sent[:5])\n",
    "\n",
    "print '\\nTo be or...\\n'\n",
    "for sent in train_sents:\n",
    "    if sent[:3] == ['To', 'be', 'or']:\n",
    "        print sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Посчитаем перплексию униграмной модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple estimator perplexity = 106.361326075\n"
     ]
    }
   ],
   "source": [
    "uni_storage = NGramStorage(train_sents, 1)\n",
    "uni_simple_estimator = StraightforwardProbabilityEstimator(uni_storage)\n",
    "print('Simple estimator perplexity = {}'.format(perplexity(uni_simple_estimator, test_sents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.19891402357e-15\n"
     ]
    }
   ],
   "source": [
    "print(uni_simple_estimator.prob('To be or not to be'.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Ответьте на следующие вопросы (внутри ipython ноутбука):\n",
    "\n",
    "**Q:** Какие выводы можно сделать? Почему $P(\\text{To be or not to be}) = 0$, хотя мы и добавили UNK токен?  \n",
    "**A:** В обучающей выборке нету этого предложения хотя бы с \"To be or\". Т.е. вероятность этого триграма 0, соответственно вся вероятность предложения = 0. А UNK мы добавляли для 1-грамов. Поэтому, если мы проверим для униграмной модели, то получим не 0\n",
    "\n",
    "**Q:** Почему перплексия униграмной модели меньше, чем триграмной?  \n",
    "**A:** Потому, что вероятность встретить только одно слово выше, чем встретить три подряд идущих слова. Если все слова из теста есть в обучающей выборке, то вероятность не 0. Соотвественно перплексия маленькая."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Add-one smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Простейший вид сглаживания — **сглаживание Лапласа**. Чтобы избавиться от нулевых вероятностей $P(w_{N} \\mid w_1^{N - 1})$, будем использовать формулу:\n",
    "\n",
    "$$\n",
    "    \\hat P_{AOS}(w_{N} \\mid w_1^{N - 1}) = \\frac{c(w_1^N) + \\delta}{c(w_1^{N-1}) + \\delta V},\n",
    "$$\n",
    "\n",
    "где $V$ — это размер словаря, а $\\delta$ — некоторая фиксированная константа."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Реализуйте класс, осуществляющий сглаживание Лапласа. Он должен иметь аналогичный интерфейс, как и StraightforwardProbabilityEstimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class LaplaceProbabilityEstimator:\n",
    "    \"\"\"Class for probability estimations of type P(word | context).\n",
    "    \n",
    "    P(word | context) = (c(context + word) + delta) / (c(context) + delta * V), where\n",
    "    c(sequence) - number of occurances of the sequence in the corpus,\n",
    "    delta - some constant,\n",
    "    V - number of different words in corpus.\n",
    "    \n",
    "    Args:\n",
    "        storage(NGramStorage): Object of NGramStorage class which will\n",
    "            be used to extract frequencies of ngrams.\n",
    "        delta(float): Smoothing parameter.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, storage, delta=1.):\n",
    "        self.__storage = storage\n",
    "        self.__delta = delta\n",
    "        \n",
    "    def cut_context(self, context):\n",
    "        \"\"\"Cut context if it is too large.\n",
    "        \n",
    "        Args:\n",
    "            context (tuple[str]): Some sequence of words.\n",
    "        \n",
    "        Returns:\n",
    "            Cutted context (tuple[str]) up to the length of max_n.\n",
    "        \"\"\"\n",
    "        if len(context) + 1 > self.__storage.max_n:\n",
    "            context = context[-self.__storage.max_n + 1:]\n",
    "        return context\n",
    "        \n",
    "    def __call__(self, word, context):\n",
    "        \"\"\"Estimate conditional probability P(word | context).\n",
    "        \n",
    "        Args:\n",
    "            word (str): Current word.\n",
    "            context (tuple[str]): Context of a word.\n",
    "            \n",
    "        Returns:\n",
    "            Conditional probability (float) P(word | context).\n",
    "        \"\"\"\n",
    "        # Cheking the input\n",
    "        if not isinstance(word, (unicode, str)):\n",
    "            raise TypeError('word must be a string!')\n",
    "        if not isinstance(context, tuple):\n",
    "            raise TypeError('context must be a tuple!')\n",
    "            \n",
    "        context = self.cut_context(context)\n",
    "        phrase_counts = self.__storage(context + (word, ))\n",
    "        context_counts = self.__storage(context)\n",
    "\n",
    "        return 1. * (phrase_counts + self.__delta) / (context_counts + self.__delta * len(self.__storage[1]))\n",
    "    \n",
    "    def prob(self, sent):\n",
    "        \"\"\"Estimate probability of a sentence using Markov rule.\n",
    "        \n",
    "        Args:\n",
    "            sentence (list[str]): Sentence for probability estimation.\n",
    "            \n",
    "        Returns:\n",
    "            Probability (float) P(sentence).\n",
    "        \"\"\"\n",
    "        prob = 1.\n",
    "        for i in range(len(sent)):\n",
    "            prob *= self(sent[i], tuple(sent[:i]))\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Подберите наилучший параметр $\\delta$ для данного корпуса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Best delta:', 0.00040817285714285717)\n",
      "Laplace estimator perplexity = 133.980012654\n",
      "1.42196692297e-05\n",
      "CPU times: user 57.5 s, sys: 233 ms, total: 57.7 s\n",
      "Wall time: 57.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "deltas = np.linspace(1e-8, 0.01, 50)\n",
    "perp = np.inf\n",
    "perps = []\n",
    "for delta in deltas:\n",
    "    tmp = perplexity(LaplaceProbabilityEstimator(storage, delta), test_sents)\n",
    "    perps += [tmp]\n",
    "    if tmp < perp:\n",
    "        perp = tmp\n",
    "        best_delta = delta\n",
    "    \n",
    "# best_delta = 1.\n",
    "print('Best delta:', best_delta)\n",
    "\n",
    "# Initialize estimator\n",
    "laplace_estimator = LaplaceProbabilityEstimator(storage, best_delta)\n",
    "\n",
    "# Let's make some estimations\n",
    "print('Laplace estimator perplexity = {}'.format(perplexity(laplace_estimator, test_sents)))\n",
    "print(laplace_estimator.prob('To be'.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeoAAAFpCAYAAABTSWtMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f875637cd10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(deltas, perps, '.-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Stupid backoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Идея **простого отката** довольно понятна. Если у нас есть достаточно информцаии для подсчета вероятности $k$-грам, то будем использовать $k$-грамы. Иначе будем использовать вероятности $(k-1)$-грам с некоторым множителем, например, $0.4$, и так далее. К сожалению, в данном случае мы получим не вероятностное распределение, но в большинстве задач это не имеет принципиального значения. Если это все же важно, то необходимо подобрать множитель соответствующим образом."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Реализуйте класс, симулирующий сглаживание простым откатом. Он должен иметь аналогичный интерфейс, как и StraightforwardProbabilityEstimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class StupidBackoffProbabilityEstimator:\n",
    "    \"\"\"Class for stupid backoff probability estimations.\n",
    "    \n",
    "    P(word | context) =\n",
    "        P'(word | context),                  if  P'(word | context) > 0;\n",
    "        P'(word | context[1:]) * multiplier, if  P'(word | context) == 0\n",
    "                                             and P'(word | context[1:]) > 0;\n",
    "        ...\n",
    "    P'(word | context) - probability of a word provided context of a base estimator.\n",
    "    \n",
    "    Args:\n",
    "        base_estimator(BaseProbabilityEstimator): Object of BaseProbabilityEstimator\n",
    "            or some other class which can estimate conditional probabilities.\n",
    "        multiplier (float): Multiplier which is used for probability estimations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_estimator, multiplier=0.1):\n",
    "        self.__base_estimator = base_estimator\n",
    "        self.__mult = multiplier\n",
    "        \n",
    "    def __call__(self, word, context):\n",
    "        \"\"\"Estimate conditional probability P(word | context).\n",
    "        \n",
    "        Args:\n",
    "            word (str): Current word.\n",
    "            context (tuple[str]): Context of a word.\n",
    "            \n",
    "        Returns:\n",
    "            Conditional probability (float) P(word | context).\n",
    "        \"\"\"\n",
    "        mult = 1.\n",
    "        prob = self.__base_estimator(word, context)\n",
    "        for i in range(len(context) + 1):\n",
    "            prob = self.__base_estimator(word, context[i:])\n",
    "            if prob > 0:\n",
    "                break\n",
    "            mult *= self.__mult\n",
    "\n",
    "        return mult * prob\n",
    "    \n",
    "    def prob(self, sent):\n",
    "        \"\"\"Estimate probability of a sentence using Markov rule.\n",
    "        \n",
    "        Args:\n",
    "            sentence (list[str]): Sentence for probability estimation.\n",
    "            \n",
    "        Returns:\n",
    "            Probability (float) P(sentence).\n",
    "        \"\"\"\n",
    "        prob = 1.\n",
    "        for i in range(len(sent)):\n",
    "            prob *= self(sent[i], tuple(sent[:i]))\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stupid backoff estimator perplexity = 121.549621152\n",
      "1.5084337609e-05\n"
     ]
    }
   ],
   "source": [
    "# Initialize estimator\n",
    "sbackoff_estimator = StupidBackoffProbabilityEstimator(simple_estimator, .4)\n",
    "\n",
    "# Let's make some estimations\n",
    "print('Stupid backoff estimator perplexity = {}'.format(perplexity(sbackoff_estimator, test_sents)))\n",
    "print(sbackoff_estimator.prob('To be'.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Ответьте на следующие вопросы (внутри ipython ноутбука):\n",
    "\n",
    "**Q:** Почему бессмысленно измерять перплексию в случае **Stupid backoff**?  \n",
    "**A:** Потому, что в этом случае мы получим не вероятностное распределение\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Interpolation smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "В данном случае идея сглаживания посредством **интерполяции** также крайне проста. Пусть у нас есть $N$-грамная модель. Заведем вектор $\\bar\\lambda = (\\lambda_1, \\dots, \\lambda_N)$, такой, что $\\sum_i\\lambda_i = 1$ и $\\lambda_i \\geq 0$. Тогда\n",
    "\n",
    "$$\n",
    "    \\hat P_{IS}(w_{N} \\mid w_1^{N-1}) = \\sum_{i=1}^N \\lambda_i \\hat P_{S}(w_N \\mid w_{N-i+1}^{N-1}).\n",
    "$$\n",
    "\n",
    "Придумайте, как обойтись одним вектором $\\bar\\lambda$, т.е. пользоваться им как в случае контекста длины $N$, так и при контексте меньшей длины (например, в начале предложения). Если мы просто обрубим сумму, то у нас уже не будет вероятностное распределение, что, конечно же, плохо."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class InterpolationProbabilityEstimator:\n",
    "    \"\"\"Class for interpolation probability estimations.\n",
    "    \n",
    "    P(word | context) =\n",
    "        lambda_N * P'(word | context) +\n",
    "        lambda_{N-1} * P'(word | context[1:]) +\n",
    "        ... +\n",
    "        lambda_1 * P'(word)\n",
    "    P'(word | context) - probability of a word provided context of a base estimator.\n",
    "    \n",
    "    Args:\n",
    "        base_estimator(BaseProbabilityEstimator): Object of BaseProbabilityEstimator\n",
    "            or some other class which can estimate conditional probabilities.\n",
    "        lambdas (np.array[float]): Lambdas which are used for probability estimations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_estimator, lambdas):\n",
    "        self.lambdas = lambdas\n",
    "        self.__base_estimator = base_estimator\n",
    "        \n",
    "    def __call__(self, word, context):\n",
    "        \"\"\"Estimate conditional probability P(word | context).\n",
    "        \n",
    "        Args:\n",
    "            word (str): Current word.\n",
    "            context (tuple[str]): Context of a word.\n",
    "            \n",
    "        Returns:\n",
    "            Conditional probability (float) P(word | context).\n",
    "        \"\"\"        \n",
    "        N = min(len(self.lambdas), len(context) + 1)\n",
    "        context = context[-N + 1:]\n",
    "        lambdas = self.lambdas[-N:][::-1]\n",
    "        lambdas = np.array(lambdas) / np.sum(lambdas)\n",
    "        prob = 0.0\n",
    "        for i in range(N):\n",
    "            prob += lambdas[i] * self.__base_estimator(word, context[i:])\n",
    "\n",
    "        return prob\n",
    "    \n",
    "    def prob(self, sent):\n",
    "        \"\"\"Estimate probability of a sentence using Markov rule.\n",
    "        \n",
    "        Args:\n",
    "            sentence (list[str]): Sentence for probability estimation.\n",
    "            \n",
    "        Returns:\n",
    "            Probability (float) P(sentence).\n",
    "        \"\"\"\n",
    "        prob = 1.\n",
    "        for i in range(len(sent)):\n",
    "            prob *= self(sent[i], tuple(sent[:i]))\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolation estimator perplexity = 91.8909123944\n",
      "1.18101057322e-05\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize estimator\n",
    "interpol_estimator = InterpolationProbabilityEstimator(simple_estimator, np.array([0.2, 0.2, 0.6]))\n",
    "\n",
    "# Let's make some estimations\n",
    "print('Interpolation estimator perplexity = {}'.format(perplexity(interpol_estimator, test_sents)))\n",
    "print(interpol_estimator.prob('To be'.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Обучить значения параметров $\\lambda$ можно с помощью EM-алгоритма, но мы не будем этого здесь делать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Kneser-Ney smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Идея данного сглаживания заключается в том, что словам, которые участвуют в большом количестве контекстов, присваиваются большие вероятности, а те, которые используются в паре-тройке контекстов, получают маленькие вероятности. Авторы данного сглаживания формализовали это следующим образом. Введем обозначения\n",
    "\n",
    "$$\n",
    "    N_{c}(w) := \\left|\\{\\hat w : c(\\hat w, w) > 0\\}\\right|\n",
    "$$\n",
    "\n",
    "$-$ число N-грамм, в которых последней частью идёт $w$ (слово или последовательность слов).\n",
    "\n",
    "Опеределим рекурентное соотношение:\n",
    "\n",
    "$$\n",
    "    \\hat P_{KN} (w_1) = \\frac{N_{c}(w_1)}{\\sum_{w} N_{c}(w)},\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\hat P_{KN}(w_{i} \\mid w_{i - n + 1}^{i-1}) = \\frac{{\\rm max}\\{c(w_{i -n +1}^i) - \\delta, 0\\}}{\\sum_{w}c(w_{i - n + 1}^{i-1}, w)} + \\lambda(w^{i-1}_{i-n+1}) \\hat P_{KN}(w_{i} \\mid w^{i-1}_{i-n+2}).\n",
    "$$\n",
    "\n",
    "где\n",
    "\n",
    "$$\n",
    "\\lambda(w^{i-1}_{i-n+1}) = \\frac{\\delta}{\\sum_{w}c(w_{i - n + 1}^{i-1}, w)}N_{c}(w_{i-n+1}^{i-1})\n",
    "$$\n",
    "\n",
    "$-$ весовой множитель.\n",
    "\n",
    "\n",
    "Реализуйте данный подход."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class KneserNeyProbabilityEstimator:\n",
    "    \"\"\"Class for probability estimations of type P(word | context).\n",
    "    \n",
    "    P(word | context) = ...\n",
    "    \n",
    "    Args:\n",
    "        storage(NGramStorage): Object of NGramStorage class which will\n",
    "            be used to extract frequencies of ngrams.\n",
    "        delta(float): KneserNey parameter.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, storage, delta=1.):\n",
    "        self.__storage = storage\n",
    "        self.__delta = delta\n",
    "        \n",
    "        # how many times there were len(prefix) + 1 sequences that have prefix in it\n",
    "        self.__prefixesVal = {i: Counter() for i in range(1, storage.max_n)}\n",
    "        for i in range(1, storage.max_n):\n",
    "            for cntxt, val in storage[i + 1].items():\n",
    "                self.__prefixesVal[i][cntxt[:i]] += val\n",
    "        \n",
    "        # how many times a postfix was the postfix of words in the corpus\n",
    "        self.__postfixesCount = {\n",
    "            i: Counter(cntxt[-i:] for cntxt in storage[i + 1].keys()) for i in range(1, storage.max_n)\n",
    "        }\n",
    "\n",
    "    def cut_context(self, context):\n",
    "        \"\"\"Cut context if it is too large.\n",
    "        \n",
    "        Args:\n",
    "            context (tuple[str]): Some sequence of words.\n",
    "        \n",
    "        Returns:\n",
    "            Cutted context (tuple[str]) up to the length of max_n.\n",
    "        \"\"\"\n",
    "        if len(context) + 1 > self.__storage.max_n:\n",
    "            context = context[-self.__storage.max_n + 1:]\n",
    "        return context\n",
    "\n",
    "    def Nc(self, postfix):\n",
    "        return self.__postfixesCount[len(postfix)][postfix]\n",
    "\n",
    "    def Sc(self, prefix):\n",
    "        return self.__prefixesVal[len(prefix)][prefix]\n",
    "\n",
    "    def prob_kn(self, word, context):\n",
    "        if len(context) == 0:\n",
    "            return 1.0 * self.Nc((word,)) / len(self.__postfixesCount[1])\n",
    "        else:\n",
    "            Sc = self.Sc(context)\n",
    "            Nc = self.Nc(context)\n",
    "            if Sc > 0:\n",
    "                mx = 1.0 * max(self.__storage(context + (word,)) - self.__delta, 0)\n",
    "                lambda_ = 1.0 * self.__delta * Nc\n",
    "                return (mx + lambda_ * self.prob_kn(word, context[1:])) / Sc\n",
    "            else:\n",
    "                return 0.0\n",
    "\n",
    "    def __call__(self, word, context):\n",
    "        \"\"\"Estimate conditional probability P(word | context).\n",
    "        \n",
    "        Args:\n",
    "            word (str): Current word.\n",
    "            context (tuple[str]): Context of a word.\n",
    "            \n",
    "        Returns:\n",
    "            Conditional probability (float) P(word | context).\n",
    "        \"\"\"\n",
    "        # Cheking the input\n",
    "        if not isinstance(word, unicode) and not isinstance(word, str):\n",
    "            raise TypeError('word must be a string!')\n",
    "        if not isinstance(context, tuple):\n",
    "            raise TypeError('word must be a tuple!')\n",
    "        # If context is too large, let's cut it.\n",
    "        context = self.cut_context(context)\n",
    "        \n",
    "        return self.prob_kn(word, context)\n",
    "    \n",
    "    def prob(self, sent):\n",
    "        \"\"\"Estimate probability of a sentence using Markov rule.\n",
    "        \n",
    "        Args:\n",
    "            sentence (list[str]): Sentence for probability estimation.\n",
    "            \n",
    "        Returns:\n",
    "            Probability (float) P(sentence).\n",
    "        \"\"\"\n",
    "        prob = 1.\n",
    "        for i in range(len(sent)):\n",
    "            p = self(sent[i], tuple(sent[:i]))\n",
    "            prob *= p\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.69 s, sys: 113 ms, total: 3.8 s\n",
      "Wall time: 3.74 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Initialize estimator\n",
    "kn_estimator = KneserNeyProbabilityEstimator(storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kneser-Ney estimator perplexity = 223.323678763\n",
      "2.26546408741e-05\n",
      "1.51558182779e-12\n"
     ]
    }
   ],
   "source": [
    "# Estimating perplexity\n",
    "print('Kneser-Ney estimator perplexity = {}'.format(perplexity(kn_estimator, test_sents)))\n",
    "print(kn_estimator.prob('To be'.split()))\n",
    "print(kn_estimator.prob('To be or not to be'.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Определение языка документа"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Постановка задачи:**  \n",
    "Одна из задач, которая может быть решена при помощи языковых моделей $-$ **определение языка документа**. Реализуйте два классификатора для определения языка документа:\n",
    "1. Наивный классификатор, который будет учитывать частоты символов и выбирать язык текста по признаку: распределение частот символов \"наиболее похоже\" на распределение частот символов в выбранном языке.\n",
    "2. Классификатор на основе языковых моделей. Сами придумайте, как он должен работать.  \n",
    "_Подсказка_: лучше считать n-грамы не по словам, а по символам.\n",
    "\n",
    "---\n",
    "\n",
    "**Как представлены данные:**  \n",
    "Во всех текстовых файлах на каждой строчке записано отдельное предложение.\n",
    "1. В папке _data_ находятся две папки: _full_ и _plain_. В _full_ находятся тексты в той форме, что они были взяты из сети, в _plain_ находятся те же самые тексты, но с них сначала была снята диакритика, а затем русский и греческий тексты были транслитерованы в английский.\n",
    "2. В каждой из папок _full_ и _plain_ находятся папки _train_ и _test_.\n",
    "3. В _train_ находятся файлы с текстами с говорящими именами, например, _ru.txt_, _en.txt_.\n",
    "4. В _test_ находятся файлы _1.txt_, _2.txt_, $\\dots$ в которых хранятся тексты, язык которых нужно определить. В этой же папке находится файл _ans.csv_, в котором вы можете найти правильные ответы и проверить, насколько хорошо сработали Ваши алгоритмы.\n",
    "\n",
    "---\n",
    "\n",
    "**Что нужно сделать:**  \n",
    "Напишите два своих классификатора (которые описаны в постановке задачи) и получите максимально возможное accuracy на test-сете. Разрешается использовать только _train_ для обучения.\n",
    "\n",
    "---\n",
    "\n",
    "**В данном задании мы не предоставляем стартового кода!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LangNGramStorage:\n",
    "    def __init__(self, base_path, list_of_filename, max_n):\n",
    "        self.__regexp = re.compile(r'([~!@#$%^&*()_+=;:\"{}\\[\\]<>,.?\\n\\r\\t|\\/0-9]| - |[\\x00-\\x1F])')\n",
    "        self.__max_n = max_n\n",
    "        self.__storage = dict()\n",
    "        for fname in list_of_filename:\n",
    "            lang = fname.split('.')[0]\n",
    "            \n",
    "            data = []\n",
    "            with open(base_path + fname, 'r') as f:\n",
    "                for line in f:\n",
    "                    line = self.regexp.sub('', line).decode('utf-8').lower()\n",
    "                    data += line.split(' ')\n",
    "\n",
    "            self.__storage[lang] = NGramStorage(data, self.__max_n)\n",
    "            \n",
    "    @property\n",
    "    def max_n(self):\n",
    "        \"\"\"Get max_n\"\"\"\n",
    "        return self.__max_n\n",
    "    \n",
    "    @property\n",
    "    def storage(self):\n",
    "        \"\"\"Get ngrams\"\"\"\n",
    "        return self.__storage\n",
    "    \n",
    "    @property\n",
    "    def regexp(self):\n",
    "        \"\"\"Get ngrams\"\"\"\n",
    "        return self.__regexp\n",
    "    \n",
    "    def __getitem__(self, lang):\n",
    "        return self.__storage[lang]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(clf, path, ans):\n",
    "    res = []\n",
    "    for i, lang in ans:\n",
    "        res += [clf.predict(path, '%s.txt' % i) == lang]\n",
    "        \n",
    "    return 100.0 * np.mean(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FULL_TRAIN_PATH = 'data/full/train/'\n",
    "FULL_TEST_PATH = 'data/full/test/'\n",
    "full_train_files = glob.glob1(FULL_TRAIN_PATH, '*.txt')\n",
    "full_ans = pd.read_csv(FULL_TEST_PATH + 'ans.csv', header=None).values\n",
    "\n",
    "PLAIN_TRAIN_PATH = 'data/plain/train/'\n",
    "PLAIN_TEST_PATH = 'data/plain/test/'\n",
    "plain_train_files = glob.glob1(PLAIN_TRAIN_PATH, '*.txt')\n",
    "plain_ans = pd.read_csv(PLAIN_TEST_PATH + 'ans.csv', header=None).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В наивном классификаторе считаются частоты символов и сравниваются по евклидовой метрике (то, что попалось под руку =)). Пространство расширяется до объединения символов, и новые измерения обнуляются. Таким образом можно применять евклидову метрику"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class NaiveClassifier:\n",
    "    def __init__(self, storage):\n",
    "        self.__storage = storage\n",
    "\n",
    "    def dist(self, p1, p2):\n",
    "        s = 0\n",
    "        for i in range(1, p1.max_n + 1):\n",
    "            syms_p1 = p1[i]\n",
    "            syms_p2 = p2[i]\n",
    "            sum_p1 = sum(syms_p1.values())\n",
    "            sum_p2 = sum(syms_p2.values())\n",
    "            for sym in set(syms_p1.keys()).union(syms_p2.keys()):\n",
    "                s += (1.0 * syms_p1[sym] / sum_p1 - 1.0 * syms_p2[sym] / sum_p2) ** 2\n",
    "\n",
    "            return np.sqrt(s)\n",
    "        \n",
    "    def predict(self, base_path, fname):\n",
    "        data = []\n",
    "        with open(base_path + fname, 'r') as f:\n",
    "            for line in f:\n",
    "                line = self.__storage.regexp.sub('', line).decode('utf-8').lower()\n",
    "                data += line.split(' ')\n",
    "\n",
    "        symbols = NGramStorage(data, self.__storage.max_n)\n",
    "        m = np.inf\n",
    "        language = None\n",
    "        for lang in self.__storage.storage.keys():\n",
    "            dist = self.dist(self.__storage[lang], symbols)\n",
    "            if dist < m:\n",
    "                m = dist\n",
    "                language = lang\n",
    "                \n",
    "        return language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### FULL: 1-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 8s, sys: 493 ms, total: 1min 9s\n",
      "Wall time: 1min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "storage_uni_full = LangNGramStorage(FULL_TRAIN_PATH, full_train_files, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точность: 100.0\n",
      "CPU times: user 3.85 s, sys: 33.3 ms, total: 3.88 s\n",
      "Wall time: 3.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = NaiveClassifier(storage_uni_full)\n",
    "print 'Точность:', accuracy(clf, FULL_TEST_PATH, full_ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### FULL: 3-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 1s, sys: 713 ms, total: 2min 2s\n",
      "Wall time: 2min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "storage_tri_full = LangNGramStorage(FULL_TRAIN_PATH, full_train_files, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точность: 100.0\n",
      "CPU times: user 6.93 s, sys: 53.3 ms, total: 6.98 s\n",
      "Wall time: 6.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = NaiveClassifier(storage_tri_full)\n",
    "print 'Точность:', accuracy(clf, FULL_TEST_PATH, full_ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PLAIN: 1-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 9s, sys: 497 ms, total: 1min 9s\n",
      "Wall time: 1min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "storage_uni_plain = LangNGramStorage(PLAIN_TRAIN_PATH, plain_train_files, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точность: 99.5833333333\n",
      "CPU times: user 3.81 s, sys: 20 ms, total: 3.83 s\n",
      "Wall time: 3.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = NaiveClassifier(storage_uni_plain)\n",
    "print 'Точность:', accuracy(clf, PLAIN_TEST_PATH, plain_ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PLAIN: 3-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 2s, sys: 747 ms, total: 2min 3s\n",
      "Wall time: 2min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "storage_tri_plain = LangNGramStorage(PLAIN_TRAIN_PATH, plain_train_files, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точность: 99.5833333333\n",
      "CPU times: user 6.48 s, sys: 40 ms, total: 6.52 s\n",
      "Wall time: 6.47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = NaiveClassifier(storage_tri_plain)\n",
    "print 'Точность:', accuracy(clf, PLAIN_TEST_PATH, plain_ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lang. model classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Классификатор основанный на языковой модели обучает языковые модели и предсказывает по минимуму перплексии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LMClassifier:\n",
    "    def __init__(self, storage):\n",
    "        self.__storage = storage\n",
    "        self.__models = dict()\n",
    "        param = np.array([0.2, 0.2, 0.6])\n",
    "        for lang, strage in storage.storage.items():\n",
    "            self.__models[lang] = StraightforwardProbabilityEstimator(strage)\n",
    "#             self.__models[lang] = InterpolationProbabilityEstimator(\n",
    "#                 StraightforwardProbabilityEstimator(strage), param\n",
    "#             )\n",
    "        \n",
    "    def predict(self, base_path, fname):\n",
    "        data = []\n",
    "        with open(base_path + fname, 'r') as f:\n",
    "            for line in f:\n",
    "                line = self.__storage.regexp.sub('', line).decode('utf-8').lower()\n",
    "                data += line.split(' ')\n",
    "\n",
    "        m = np.inf\n",
    "        language = None\n",
    "        for lang in self.__storage.storage.keys():\n",
    "            dist = perplexity(self.__models[lang], data)\n",
    "            if dist < m:\n",
    "                m = dist\n",
    "                language = lang\n",
    "                \n",
    "        return language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### FULL: 1-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точность: 100.0\n",
      "CPU times: user 2min 30s, sys: 507 ms, total: 2min 31s\n",
      "Wall time: 2min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = LMClassifier(storage_uni_full)\n",
    "print 'Точность:', accuracy(clf, FULL_TEST_PATH, full_ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### FULL: 3-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точность: 99.5833333333\n",
      "CPU times: user 2min 42s, sys: 547 ms, total: 2min 43s\n",
      "Wall time: 2min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = LMClassifier(storage_tri_full)\n",
    "print 'Точность:', accuracy(clf, FULL_TEST_PATH, full_ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### PLAIN: 1-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точность: 100.0\n",
      "CPU times: user 2min 26s, sys: 513 ms, total: 2min 27s\n",
      "Wall time: 2min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = LMClassifier(storage_uni_plain)\n",
    "print 'Точность:', accuracy(clf, PLAIN_TEST_PATH, plain_ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### PLAIN: 3-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точность: 100.0\n",
      "CPU times: user 2min 40s, sys: 553 ms, total: 2min 41s\n",
      "Wall time: 2min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = LMClassifier(storage_tri_plain)\n",
    "print 'Точность:', accuracy(clf, PLAIN_TEST_PATH, plain_ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наивная модель работает отлично и к тому же очень быстро <br>\n",
    "Языковая модель работает лучше, но достаточно медленно <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th>Данные</th>\n",
    "        <th>Словарь</th>\n",
    "        <th>Наивный классификатор</th>\n",
    "        <th>Классификатор на языкаовых моделях</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th colspan=2></th>\n",
    "        <th colspan=3 style=\"text-align: center\">точность, %</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td rowspan=2>FULL</td>\n",
    "        <td>unigram</td>\n",
    "        <td>100</td>\n",
    "        <td>100</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>trigram</td>\n",
    "        <td>100</td>\n",
    "        <td>99.6</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td rowspan=2>PLAIN</td>\n",
    "        <td>unigram</td>\n",
    "        <td>99.6</td>\n",
    "        <td>100</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>trigram</td>\n",
    "        <td>99.6</td>\n",
    "        <td>100</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th colspan=2></th>\n",
    "        <th colspan=3 style=\"text-align: center\">время</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td rowspan=2>FULL</td>\n",
    "        <td>unigram</td>\n",
    "        <td>3.84 s</td>\n",
    "        <td>2min 30s</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>trigram</td>\n",
    "        <td>6.9 s</td>\n",
    "        <td>2min 42s</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td rowspan=2>PLAIN</td>\n",
    "        <td>unigram</td>\n",
    "        <td>3.8 s</td>\n",
    "        <td>2min 26s</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>trigram</td>\n",
    "        <td>6.47 s</td>\n",
    "        <td>2min 40s</td>\n",
    "    </tr>\n",
    "\n",
    "</table>\n",
    "P.S. 99.6% точности означает, что не был правильно найден язык одного текста из 240"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
